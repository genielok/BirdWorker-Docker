import boto3
import json
import os
import time
import math
import urllib.parse

# --- 1. åŸºç¡€é…ç½® ---
SQS_QUEUE_URL = os.environ.get("SQS_QUEUE_URL")
AWS_REGION = os.environ.get("AWS_REGION", "eu-north-1")
S3_BUCKET_NAME = os.environ.get("S3_BUCKET_NAME")

# --- 2. AWS ç½‘ç»œé…ç½® (Fargate å¿…éœ€) ---
ECS_CLUSTER = os.environ.get("ECS_CLUSTER", "bird-analysis-cluster")
SUBNET_ID = os.environ.get("SUBNET_ID")
SECURITY_GROUP_ID = os.environ.get("SECURITY_GROUP_ID")

# --- 3. ä»»åŠ¡å®šä¹‰ (Task Definitions) ---
TASK_DEF_BIRDNET = os.environ.get("TASK_DEF_BIRDNET", "birdnet-task:1")
TASK_DEF_PERCH = os.environ.get("TASK_DEF_PERCH", "perch-task:1")
TASK_DEF_AGGREGATOR = os.environ.get("TASK_DEF_AGGREGATOR", TASK_DEF_BIRDNET)
CONTAINER_NAME_AGGREGATOR = os.environ.get(
    "CONTAINER_NAME_AGGREGATOR", "birdnet-worker"
)

# åˆå§‹åŒ–å®¢æˆ·ç«¯
sqs = boto3.client("sqs", region_name=AWS_REGION)
s3 = boto3.client("s3", region_name=AWS_REGION)
ecs = boto3.client("ecs", region_name=AWS_REGION)


def launch_analysis_task(model_type, project_name, file_batch, batch_index):
    if model_type == "perch":
        task_def = TASK_DEF_PERCH
        output_prefix = f"results/{project_name}/perch"
        container_name = "perch-worker"
    else:
        task_def = TASK_DEF_BIRDNET
        output_prefix = f"results/{project_name}/birdnet"
        container_name = "birdnet-worker"

    print(
        f"ğŸš€ [Batch {batch_index}] Launching {model_type} task ({len(file_batch)} files)..."
    )

    input_keys_json = json.dumps([{"key": k} for k in file_batch])

    try:
        ecs.run_task(
            cluster=ECS_CLUSTER,
            taskDefinition=task_def,
            launchType="FARGATE",
            networkConfiguration={
                "awsvpcConfiguration": {
                    "subnets": [SUBNET_ID],
                    "securityGroups": [SECURITY_GROUP_ID],
                    "assignPublicIp": "ENABLED",
                }
            },
            overrides={
                "containerOverrides": [
                    {
                        "name": container_name,
                        "environment": [
                            {"name": "S3_BUCKET_NAME", "value": S3_BUCKET_NAME},
                            {"name": "PROJECT_NAME", "value": project_name},
                            {"name": "MODEL_NAME", "value": model_type},
                            {"name": "S3_OUTPUT_PREFIX", "value": output_prefix},
                            {"name": "S3_INPUT_KEYS", "value": input_keys_json},
                        ],
                    }
                ]
            },
        )
    except Exception as e:
        print(f"ğŸ’¥ Failed to launch analysis task: {e}")


def launch_aggregator_task(project_name, total_files):
    print(f"ğŸ‘€ Launching Aggregator Task (TaskDef: {TASK_DEF_AGGREGATOR})...")

    try:
        ecs.run_task(
            cluster=ECS_CLUSTER,
            taskDefinition=TASK_DEF_AGGREGATOR,
            launchType="FARGATE",
            networkConfiguration={
                "awsvpcConfiguration": {
                    "subnets": [SUBNET_ID],
                    "securityGroups": [SECURITY_GROUP_ID],
                    "assignPublicIp": "ENABLED",
                }
            },
            overrides={
                "containerOverrides": [
                    {
                        "name": CONTAINER_NAME_AGGREGATOR,
                        "command": ["python", "aggregator.py"],
                        "environment": [
                            {"name": "S3_BUCKET_NAME", "value": S3_BUCKET_NAME},
                            {"name": "PROJECT_NAME", "value": project_name},
                            {"name": "TOTAL_FILES", "value": str(total_files)},
                            {"name": "EXPECTED_MODELS", "value": "birdnet,perch"},
                        ],
                    }
                ]
            },
        )
        print("âœ… Aggregator launched!")
    except Exception as e:
        print(f"ğŸ’¥ Failed to launch Aggregator: {e}")


def process_manifest(manifest_key):
    print(f"ğŸ“„ Processing manifest: {manifest_key}")

    try:
        obj = s3.get_object(Bucket=S3_BUCKET_NAME, Key=manifest_key)
        manifest = json.loads(obj["Body"].read())

        project_name = manifest.get("project_name", "unknown_project")
        all_files = manifest.get("audio_files", [])
        total_files = len(all_files)

        if not all_files:
            print("âš ï¸ Empty manifest, skipping.")
            return

        print(f"ğŸ“Š Project: {project_name} | Total Files: {total_files}")

        BATCH_SIZE = 50
        total_batches = math.ceil(total_files / BATCH_SIZE)

        for i in range(total_batches):
            start = i * BATCH_SIZE
            end = start + BATCH_SIZE
            batch_files = all_files[start:end]

            launch_analysis_task("birdnet", project_name, batch_files, i + 1)
            launch_analysis_task("perch", project_name, batch_files, i + 1)

        launch_aggregator_task(project_name, total_files)

    except Exception as e:
        print(f"âŒ Processing failed: {e}")


def poll_queue():
    print(f"Worker listening on: {SQS_QUEUE_URL}")
    while True:
        try:
            response = sqs.receive_message(
                QueueUrl=SQS_QUEUE_URL, MaxNumberOfMessages=1, WaitTimeSeconds=20
            )
            if "Messages" in response:
                for msg in response["Messages"]:
                    receipt_handle = msg["ReceiptHandle"]
                    try:
                        # å°è¯•è§£æ JSON
                        body_str = msg["Body"]
                        body = json.loads(body_str)

                        if "Records" in body:
                            for record in body["Records"]:
                                if "s3" in record:
                                    key = urllib.parse.unquote_plus(
                                        record["s3"]["object"]["key"]
                                    )
                                    if key.endswith("manifest.json"):
                                        process_manifest(key)

                        # æˆåŠŸå¤„ç†ï¼ˆæˆ–è€…æ˜¯åˆæ³•çš„ S3 äº‹ä»¶ä½†ä¸æ˜¯ manifestï¼‰ï¼Œåˆ é™¤æ¶ˆæ¯
                        sqs.delete_message(
                            QueueUrl=SQS_QUEUE_URL, ReceiptHandle=receipt_handle
                        )

                    except json.JSONDecodeError:
                        print(f"âŒ æ”¶åˆ°é JSON æ¶ˆæ¯: {msg['Body']}")
                        print("ğŸ—‘ï¸ è¿™æ˜¯ä¸€ä¸ªæ— æ•ˆæ¶ˆæ¯ï¼Œæ­£åœ¨åˆ é™¤ä»¥é˜²æ­¢æ­»å¾ªç¯...")
                        # å…³é”®ï¼šåˆ é™¤åæ¶ˆæ¯
                        sqs.delete_message(
                            QueueUrl=SQS_QUEUE_URL, ReceiptHandle=receipt_handle
                        )

                    except Exception as inner_e:
                        print(f"âš ï¸ å¤„ç†æ¶ˆæ¯é€»è¾‘å‡ºé”™: {inner_e}")
                        # è¿™é‡Œä¸åˆ é™¤æ¶ˆæ¯ï¼Œè®© SQS é‡è¯• (Visibility Timeout)

        except Exception as e:
            print(f"Polling connection error: {e}")
            time.sleep(5)


if __name__ == "__main__":
    missing_vars = []
    if not SQS_QUEUE_URL:
        missing_vars.append("SQS_QUEUE_URL")
    if not SUBNET_ID:
        missing_vars.append("SUBNET_ID")
    if not S3_BUCKET_NAME:
        missing_vars.append("S3_BUCKET_NAME")

    if missing_vars:
        print(f"âŒ Fatal: Missing environment variables: {', '.join(missing_vars)}")
        exit(1)
    else:
        poll_queue()
